{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours Algorithm\n",
    "\n",
    "It's a simple way to __classify__ data.\n",
    "\n",
    "As an example, we will be classifying cell types.\n",
    "\n",
    "Here is a chart that contains 10 different cell types. They are all clustered together using PCA (Principal Component Analysis).\n",
    "\n",
    "<img src=\"pic9.png\" style=\"width:400px;height:350px\"/>\n",
    "\n",
    "Now we add a new cell type to the PCA chart. But we do not know what type of cell this is, how might we classify it?\n",
    "\n",
    "We will classify the new cell by looking at its \"K\" nearest neighbours, K being any positive number. If K was 1, we would look at this cell's immediate closet neighbour, and then we use the neighbour to define the cell type.\n",
    "\n",
    "\n",
    "<img src=\"pic10.png\" style=\"width:400px;height:350px\"/>\n",
    "\n",
    "\n",
    "Even if we made \"K\" equal to 11, the 11 closest neighbours are still the green cell type, so we would still classify the unknown cell as __green__.\n",
    "\n",
    "<img src=\"pic11.png\" style=\"width:400px;height:350px\"/>\n",
    "\n",
    "\n",
    "But what if we add our new cell in a more interesting place?\n",
    "\n",
    "\n",
    "<img src=\"pic12.png\" style=\"width:400px;height:350px\"/>\n",
    "\n",
    "\n",
    "If we set K to 11, we would have:\n",
    "\n",
    "- 7 nearest neighbours are __red__\n",
    "- 3 nearest neighbours are __orange__\n",
    "- 1 nearest neighbour is __green__\n",
    "\n",
    "Since red has the most amount of neighbours, we will classify the new cell as __red__.\n",
    "\n",
    "If there is a __tie__ in the amount of neighbours, we would either flip a coin, or not classify the new cell at all.\n",
    "\n",
    "How would we choose a value for K?\n",
    "\n",
    "There is no best way to find the value for K. But what you can do is pretend some of your training data is 'unknown' and then when applying K-nearest neighbour, see if it classifies to what it actually is.\n",
    "\n",
    "Low values for K (like 1 or 2), are not very good as they are subject to the effect of outliers.\n",
    "\n",
    "Large values for K are better, but you don't want it to be so large so that a cluster/category with a small amount of samples would always get voted out by larger groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
